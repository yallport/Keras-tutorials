{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 1 - What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Artificial Intelligence, Machine Learning, and Deep Learning\n",
    "\n",
    "\n",
    "![AI](Images/01_01.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Artificial Intelligence\n",
    "\n",
    "\n",
    "AI is the effort to automate intellectual tasks normally performed by humans. \n",
    "\n",
    "AI is a general field that encompasses machine learning and deep learning, but that also includes other approaches that don't involve any learning.\n",
    "\n",
    "Born in the 1950s. Early chess programs only involved hardcoded rules crafted by programmers, and didn't qualify as machine learning. At the time experts believed that human-level artificial intelligence could only be achieved by having programmers handcraft a sufficiently large set of explicit rules. This approach is called **symbolic AI**, and was the dominant paradigm in AI to the late 1980s. \n",
    "\n",
    "Although symbolic AI proved suitable to solve well-defined, logical problems, it turned out to be intractable to figure out explicit rules for solving more complex problems such as image classification, speech recognition, and language translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Machine Learning\n",
    "\n",
    "A machine learning system is trained rather than explicitly programmed. It is presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task.\n",
    "\n",
    "For example, if you wanted to automate the task of tagging your vacation pictures, you could present a machine learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\n",
    "\n",
    "Machine learning is tightly related to stats, but unlike stats, machine learning tends to deal with large, complex datasets for which classical statistical analysis such as Bayesian analysis would be impractical.\n",
    "\n",
    "Ideas are proven empirically more often than theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Learning Representations from Data\n",
    "\n",
    "To do machine learning, we need\n",
    "\n",
    "- Input data points: if the task is speech recognition, these data points could be sound files of people speaking. If image tagging, pictures.\n",
    "\n",
    "- Examples of the expected output: in speech recognition, these could be human-generated transcripts of sound files. In an image tag, expected outputs could be tags such as 'dog,' 'cat,' and so on.\n",
    "\n",
    "- A way to measure whether the algorithm is doing a good job: this is necessary in order to determine the distance between the algorithm's current output and its expected output. The measurement is used as a feedback signal to adjust the way the algorithm works. This adjustment is *learning*.\n",
    "\n",
    "\n",
    "A machine-learning model transforms its input data into meaningful outputs, a process that is learned from exposure to known examples of inputs and outputs. The central problem in machine learning and deep learning is to meaningfully transform data: to learn useful representations of the input data at hand that get us closer to the expected output.\n",
    "\n",
    "A representation  is a different way to look at data - to represent or encode data.\n",
    "- A color image can be encoded in the RGB format or in the HSV format, these are two different representations of the same data. Some tasks that are difficult with one may be easier with the other. \n",
    "\n",
    "Machine learning models are all about finding appropriate representations for their input data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we need a representation of our data that cleanly separates the white points from the black points. One transformation is a coordinate change. This new representation basically solves the classification problem.\n",
    "\n",
    "![Representation](Images/01_02.jpg)\n",
    "\n",
    "- The inputs are the coordinates of the points\n",
    "\n",
    "- The expected outputs are the colors of our points\n",
    "\n",
    "- A way to measure whether our algorithm is doing a good job could be the percentage of points that are correctly classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 The \"Deep\" in Deep Learning\n",
    "\n",
    "Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations.\n",
    "\n",
    "Modern deep learning often involves tens or even hundreds of successive layers of representations and they’re all learned automatically from exposure to training data. Meanwhile, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; they’re sometimes called shallow learning. \n",
    "\n",
    "How many layers contribute to a model of the data is called the depth of the model.\n",
    "\n",
    "In deep learning, these layered representations are (almost always) learned via models called neural networks, structured in layers stacked on top of each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognizing Digits:\n",
    "\n",
    "Several layer deep network that will transform an image of a digit in order to recognize what digit it is: \n",
    "\n",
    "![Digits1](Images/01_03.jpg)\n",
    "\n",
    "\n",
    "As you can see in figure 1.6, the network transforms the digit image into representations that are increasingly different from the original image and increasingly informative about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task).\n",
    "\n",
    "\n",
    "![Digits2](Images/01_04.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Understanding How Deep Learning Works, In 3 Figures\n",
    "\n",
    "The specification of what a layer does to its input data is stored in the layer's weights. The transformation implemented by a layer is parameterized by its weights. \n",
    "\n",
    "Learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets. \n",
    "\n",
    "\n",
    "![DeepLearning1](Images/01_05.jpg)\n",
    "\n",
    "\n",
    "To control the output of a neural network, you need to be able to measure how far this output is from what you expected. That is the job of the loss function.\n",
    "\n",
    "\n",
    "![DeepLearning2](Images/01_06.jpg)\n",
    "\n",
    "\n",
    "The trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example. \n",
    "\n",
    "This adjustment is the job of the optimizer, which implements what's called the Backpropagation algorithm: the central algorithm in deep learning. \n",
    "\n",
    "![DeepLearning3](Images/01_07.jpg)\n",
    "\n",
    "\n",
    "Initially, the weights are random. With every example, the network adjusts, and the loss becomes smaller. This is the training loop, which when repeated, yields weight values that minimize the loss function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 What Deep Learning Has Achieved So Far\n",
    "\n",
    "Deep learning has achieved these things:\n",
    "\n",
    "- Near human-level image classification\n",
    "\n",
    "- Near human-level speech recognition\n",
    "\n",
    "- Near-human-level handwriting transcription\n",
    "\n",
    "- Improved machine translation\n",
    "\n",
    "- Improved text-to-speech conversion\n",
    "\n",
    "- Digital assistants\n",
    "\n",
    "- Near-human-level autonomous driving\n",
    "\n",
    "- Improved ad targeting\n",
    "\n",
    "- Improved search results\n",
    "\n",
    "- Ability to answer natural-language questions\n",
    "\n",
    "- Superhuman Go playing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 Don't Believe the Short-Term Hype\n",
    "\n",
    "Most expectations tend to be higher than is actually possible in the near future. Eventually progress will slow again, like it did in the 60s and 90s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 The Promise of AI\n",
    "\n",
    "Long-term, future is looking bright, especially as people begin to recognize the possibilities that AI can bring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Before Deep Learning: A Brief History of Machine Learning\n",
    "\n",
    "Deep learning isn't always the right tool for the job - sometimes there isn't enough data, sometimes the problem is better solved with a different algorithm. \n",
    "\n",
    "Learn different approaches so you don't try to tackle every machine learning problem with deep learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Probabilistic Thinking\n",
    "\n",
    "\n",
    "Probabilistic modeling is the application of the principles of statistics to data analysis. It was one of the earliest forms of machine learning, and it’s still widely used to this day. One of the best-known algorithms in this category is the Naive Bayes algorithm.\n",
    "\n",
    "Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theorem while assuming that the features in the input data are all independent (a strong, or “naive” assumption, which is where the name comes from). Bayes’ theorem and the foundations of statistics date back to the eighteenth century, and these are all you need to start using Naive Bayes classifiers.\n",
    "\n",
    "A closely related model is the logistic regression (logreg for short), which is sometimes considered to be the “hello world” of modern machine learning. Don’t be misled by its name—logreg is a classification algorithm rather than a regression\n",
    "algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Early Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "Early iterations of neural networks have been completely supplanted by the modern variants covered in these pages, but it’s helpful to be aware of how deep learning originated. Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to get started. For a long time, the missing piece was an efficient way to train large neural networks. \n",
    "\n",
    "\n",
    "This changed in the mid-1980s, when multiple people independently rediscovered the Backpropagation algorithm — a way to train chains of parametric operations using gradient-descent optimization and started applying it to neural networks.\n",
    "\n",
    "\n",
    "The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits. The resulting network, dubbed LeNet, was used by the United States Postal Service in the 1990s to automate the reading of ZIP codes on mail envelopes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Kernel Methods\n",
    "\n",
    "\n",
    "Kernel methods are a group of classification algorithms, the best known of which is the support vector machine (SVM).\n",
    "\n",
    "\n",
    "SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories. A decision boundary can be thought of as a line or surface separating your training data into two spaces corresponding to two categories. To classify new data points, you just need to check which side of the decision boundary they fall on.\n",
    "\n",
    "\n",
    "\n",
    "![Decision Boundary](Images/01_08.jpg)\n",
    "\n",
    "\n",
    "\n",
    "SVMs find these boundaries in two steps:\n",
    "\n",
    "- The data is mapped to a new high-dimensional representation where the decision boundary can be expressed as a hyperplane (if the data was two-dimensional, a hyperplane would be a straight line).\n",
    "\n",
    "- A good decision boundary (a separation hyperplane) is computed by trying to maximize the distance between the hyperplane and the closest data points from each class, a step called maximizing the margin. This allows the boundary to generalize well to new samples outside of the training dataset.\n",
    "\n",
    "\n",
    "\n",
    "**The kernel trick** - to find a good decision hyperplanes in the new representation space, you don't have to explicitly compute the coordinates of your points in the new space; you just need to compute the distance between two points in that space, which can be done efficiently using a kernel function. \n",
    "\n",
    "A kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target representation space, completely bypassing the explicit computation of the new representation. Kernel functions are typically crafted by hand rather than learned from data - in the case of an SVM, only the separation hyperplane is learned.\n",
    "\n",
    "SVMs were popular, but proved hard to scale to large datasets and didn't provide good results for perceptual problems such as image classification. Because an SVM is a shallow method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Decision Trees, Random Forests, and Gradient Boosting Machines\n",
    "\n",
    "Decision trees are flowchart-like structures that let you classify input data points or predict output values given inputs (see figure 1.11). They’re easy to visualize and interpret. Decisions trees learned from data began to receive significant research interest in the 2000s, and by 2010 they were often preferred to kernel methods.\n",
    "\n",
    "\n",
    "![1.2.4](Images/01_09.jpg)\n",
    "\n",
    "\n",
    "In particular, the Random Forest algorithm introduced a robust, practical take on decision-tree learning that involves building a large number of specialized decision trees and then ensembling their outputs. Random forests are applicable to a wide range of problems—you could say that they’re almost always the second-best algorithm for any shallow machine-learning task.\n",
    "\n",
    "\n",
    "A gradient boosting machine, much like a random forest, is a machine-learning technique based on ensembling weak prediction models, generally decision trees. It uses gradient boosting, a way to improve any machine-learning model by iteratively training new models that specialize in addressing the weak points of the previous models. Applied to decision trees, the use of the gradient boosting technique results in models that strictly outperform random forests most of the time, while having similar properties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual data today.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Back to Neural Networks\n",
    "\n",
    "\n",
    "In 2011, Dan Ciresan from IDSIA began to win academic image-classification competitions with GPU-trained deep neural networks—the first practical success of modern deep learning. But the watershed moment came in 2012, with the entry of Hinton’s group in the yearly large-scale image-classification challenge ImageNet. The\n",
    "ImageNet challenge was notoriously difficult at the time, consisting of classifying highresolution color images into 1,000 different categories after training on 1.4 million images. In 2011, the top-five accuracy of the winning model, based on classical approaches to computer vision, was only 74.3%. Then, in 2012, a team led by Alex\n",
    "Krizhevsky and advised by Geoffrey Hinton was able to achieve a top-five accuracy of 83.6%—a significant breakthrough. The competition has been dominated by deep convolutional neural networks every year since. By 2015, the winner reached an accuracy of 96.4%, and the classification task on ImageNet was considered to be a completely solved problem.\n",
    "\n",
    "\n",
    "Since 2012, deep convolutional neural networks (convnets) have become the go-to algorithm for all computer vision tasks; more generally, they work on all perceptual tasks. At major computer vision conferences in 2015 and 2016, it was nearly impossible to find presentations that didn’t involve convnets in some form. At the same time, deep learning has also found applications in many other types of problems, such as natural-language processing. It has completely replaced SVMs and decision trees in a wide range of applications. For instance, for several years, the European Organization for Nuclear Research, CERN, used decision tree–based methods for analysis of particle data from the ATLAS detector at the Large Hadron Collider (LHC); but CERN eventually switched to Keras-based deep neural networks due to their higher performance and ease of training on large datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 What Makes Deep Learning Different\n",
    "\n",
    "\n",
    "Deep learning makes problem solving much easier because it completely automates what used to be the most crucial step in a machine-learning workflow: feature engineering.\n",
    "\n",
    "\n",
    "Previous machine-learning techniques—shallow learning—only involved transforming the input data into one or two successive representation spaces, usually via simple transformations such as high-dimensional non-linear projections (SVMs) or decision trees. But the refined representations required by complex problems generally can’t be attained by such techniques. As such, humans had to go to great lengths to make the initial input data more amenable to processing by these methods: they had to manually engineer good layers of representations for their data. This is called feature engineering. Deep learning, on the other hand, completely automates this step: with deep learning, you learn all features in one pass rather than having to engineer them yourself. This has greatly simplified machine-learning workflows, often replacing sophisticated multistage pipelines with a single, simple, end-to-end deep-learning model.\n",
    "\n",
    "\n",
    "\n",
    "Could shallow methods be applied repeatedly to emulate the effects of\n",
    "deep learning? In practice, there are fast-diminishing returns to successive applications of shallow-learning methods, because the optimal first representation layer in a threelayer model isn’t the optimal first layer in a one-layer or two-layer model. What is transformative about deep learning is that it allows a model to learn all layers of representation jointly, at the same time, rather than in succession (greedily, as it’s called). With joint feature learning, whenever the model adjusts one of its internal features, all other features that depend on it automatically adapt to the change, without requiring human intervention. Everything is supervised by a single feedback signal: every change in the model serves the end goal. This is much more powerful than greedily stacking shallow models, because it allows for complex, abstract representations to be learned by breaking them down into long series of intermediate spaces (layers); each space is only a simple transformation away from the previous one.\n",
    "\n",
    "\n",
    "\n",
    "These are the two essential characteristics of how deep learning learns from data: the incremental, layer-by-layer way in which increasingly complex representations are developed, and the fact that these intermediate incremental representations are learned jointly, each layer being updated to follow both the representational needs of the layer above and the needs of the layer below. Together, these two properties have made deep learning vastly more successful than previous approaches to machine learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 The Modern Machine-Learning Landscape\n",
    "\n",
    "Due to its highly competitive environment (some contests have thousands of entrants and milliondollar prizes) and to the wide variety of machine-learning problems covered, Kaggle offers a realistic way to assess what works and what doesn’t. \n",
    "\n",
    "\n",
    "In 2016 and 2017, Kaggle was dominated by two approaches: gradient boosting machines and deep learning. Specifically, gradient boosting is used for problems where structured data is available, whereas deep learning is used for perceptual problems such as image classification. Practitioners of the former almost always use the excellent XGBoost library, which offers support for the two most popular languages of data science: Python and R. Meanwhile, most of the Kaggle entrants using deep learning use the Keras library, due to its ease of use, flexibility, and support of Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Why Deep Learning? Why Now?\n",
    "\n",
    "\n",
    "The two key ideas of deep learning for computer vision—convolutional neural networks and backpropagation—were already well understood in 1989. These technical forces are driving advances in machine learning:\n",
    "\n",
    "- Hardware\n",
    "\n",
    "- Datasets and benchmarks\n",
    "\n",
    "- Algorithmic advances\n",
    "\n",
    "\n",
    "\n",
    "Because the field is guided by experimental findings rather than by theory, algorithmic advances only become possible when appropriate data and hardware are available to try new ideas (or scale up old ideas, as is often the case). Machine learning isn’t mathematics or physics, where major advances can be done with a pen and a piece of paper. It’s an engineering science.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Hardware\n",
    "\n",
    "\n",
    "\n",
    "Typical deep-learning models used in computer vision or speech recognition require orders of magnitude more computational power than what your laptop can deliver. Throughout the 2000s, companies like NVIDIA and AMD have been investing billions of dollars in developing fast, massively parallel chips (graphical processing units [GPUs]) to power the graphics of increasingly photorealistic video games—\n",
    "cheap, single-purpose supercomputers designed to render complex 3D scenes on your screen in real time. This investment came to benefit the scientific community when, in 2007, NVIDIA launched CUDA, a programming interface for its line of GPUs. A small number of GPUs started replacing massive clusters of CPUs in various highly parallelizable applications, beginning with physics modeling. Deep neural networks, consisting mostly of many small matrix multiplications, are also highly parallelizable; and around 2011, some researchers began to write CUDA implementations of neural nets—Dan Ciresan and Alex Krizhevsky were among the first.\n",
    "\n",
    "\n",
    "What happened is that the gaming market subsidized supercomputing for the next generation of artificial intelligence applications. Sometimes, big things begin as games. Today, the NVIDIA TITAN X, a gaming GPU that cost $1,000 at the end of 2015, can deliver a peak of 6.6 TFLOPS in single precision: 6.6 trillion float32 operations per second. That’s about 350 times more than what you can get out of a modern laptop. On a TITAN X, it takes only a couple of days to train an ImageNet model of the sort that would have won the ILSVRC competition a few years ago. Meanwhile, large companies train deep-learning models on clusters of hundreds of GPUs of a type developed specifically for the needs of deep learning, such as the NVIDIA Tesla K80. The sheer computational power of such clusters is something that would never have been possible without modern GPUs.\n",
    "\n",
    "\n",
    "The deep-learning industry is starting to go beyond GPUs and is investing in increasingly specialized, efficient chips for deep learning. In 2016, at its annual I/O convention, Google revealed its tensor processing unit (TPU) project: a new chip design developed from the ground up to run deep neural networks, which is reportedly 10 times faster and far more energy efficient than top-of-the-line GPUs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 A New Wave of Investment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 The Democratization of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Will It Last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
