{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 1 - What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Artificial Intelligence, Machine Learning, and Deep Learning\n",
    "\n",
    "\n",
    "![AI](Images/01_01.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Artificial Intelligence\n",
    "\n",
    "\n",
    "AI is the effort to automate intellectual tasks normally performed by humans. \n",
    "\n",
    "AI is a general field that encompasses machine learning and deep learning, but that also includes other approaches that don't involve any learning.\n",
    "\n",
    "Born in the 1950s. Early chess programs only involved hardcoded rules crafted by programmers, and didn't qualify as machine learning. At the time experts believed that human-level artificial intelligence could only be achieved by having programmers handcraft a sufficiently large set of explicit rules. This approach is called **symbolic AI**, and was the dominant paradigm in AI to the late 1980s. \n",
    "\n",
    "Although symbolic AI proved suitable to solve well-defined, logical problems, it turned out to be intractable to figure out explicit rules for solving more complex problems such as image classification, speech recognition, and language translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Machine Learning\n",
    "\n",
    "A machine learning system is trained rather than explicitly programmed. It is presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task.\n",
    "\n",
    "For example, if you wanted to automate the task of tagging your vacation pictures, you could present a machine learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\n",
    "\n",
    "Machine learning is tightly related to stats, but unlike stats, machine learning tends to deal with large, complex datasets for which classical statistical analysis such as Bayesian analysis would be impractical.\n",
    "\n",
    "Ideas are proven empirically more often than theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Learning Representations from Data\n",
    "\n",
    "To do machine learning, we need\n",
    "\n",
    "- Input data points: if the task is speech recognition, these data points could be sound files of people speaking. If image tagging, pictures.\n",
    "\n",
    "- Examples of the expected output: in speech recognition, these could be human-generated transcripts of sound files. In an image tag, expected outputs could be tags such as 'dog,' 'cat,' and so on.\n",
    "\n",
    "- A way to measure whether the algorithm is doing a good job: this is necessary in order to determine the distance between the algorithm's current output and its expected output. The measurement is used as a feedback signal to adjust the way the algorithm works. This adjustment is *learning*.\n",
    "\n",
    "\n",
    "A machine-learning model transforms its input data into meaningful outputs, a process that is learned from exposure to known examples of inputs and outputs. The central problem in machine learning and deep learning is to meaningfully transform data: to learn useful representations of the input data at hand that get us closer to the expected output.\n",
    "\n",
    "A representation  is a different way to look at data - to represent or encode data.\n",
    "- A color image can be encoded in the RGB format or in the HSV format, these are two different representations of the same data. Some tasks that are difficult with one may be easier with the other. \n",
    "\n",
    "Machine learning models are all about finding appropriate representations for their input data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we need a representation of our data that cleanly separates the white points from the black points. One transformation is a coordinate change. This new representation basically solves the classification problem.\n",
    "\n",
    "![Representation](Images/01_02.jpg)\n",
    "\n",
    "- The inputs are the coordinates of the points\n",
    "\n",
    "- The expected outputs are the colors of our points\n",
    "\n",
    "- A way to measure whether our algorithm is doing a good job could be the percentage of points that are correctly classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 The \"Deep\" in Deep Learning\n",
    "\n",
    "Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations.\n",
    "\n",
    "Modern deep learning often involves tens or even hundreds of successive layers of representations and they’re all learned automatically from exposure to training data. Meanwhile, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; they’re sometimes called shallow learning. \n",
    "\n",
    "How many layers contribute to a model of the data is called the depth of the model.\n",
    "\n",
    "In deep learning, these layered representations are (almost always) learned via models called neural networks, structured in layers stacked on top of each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognizing Digits:\n",
    "\n",
    "Several layer deep network that will transform an image of a digit in order to recognize what digit it is: \n",
    "\n",
    "![Digits1](Images/01_03.jpg)\n",
    "\n",
    "\n",
    "As you can see in figure 1.6, the network transforms the digit image into representations that are increasingly different from the original image and increasingly informative about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task).\n",
    "\n",
    "\n",
    "![Digits2](Images/01_04.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Understanding How Deep Learning Works, In 3 Figures\n",
    "\n",
    "The specification of what a layer does to its input data is stored in the layer's weights. The transformation implemented by a layer is parameterized by its weights. \n",
    "\n",
    "Learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets. \n",
    "\n",
    "\n",
    "![DeepLearning1](Images/01_05.jpg)\n",
    "\n",
    "\n",
    "To control the output of a neural network, you need to be able to measure how far this output is from what you expected. That is the job of the loss function.\n",
    "\n",
    "\n",
    "![DeepLearning2](Images/01_06.jpg)\n",
    "\n",
    "\n",
    "The trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example. \n",
    "\n",
    "This adjustment is the job of the optimizer, which implements what's called the Backpropagation algorithm: the central algorithm in deep learning. \n",
    "\n",
    "![DeepLearning3](Images/01_07.jpg)\n",
    "\n",
    "\n",
    "Initially, the weights are random. With every example, the network adjusts, and the loss becomes smaller. This is the training loop, which when repeated, yields weight values that minimize the loss function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 What Deep Learning Has Achieved So Far\n",
    "\n",
    "Deep learning has achieved these things:\n",
    "\n",
    "- Near human-level image classification\n",
    "\n",
    "- Near human-level speech recognition\n",
    "\n",
    "- Near-human-level handwriting transcription\n",
    "\n",
    "- Improved machine translation\n",
    "\n",
    "- Improved text-to-speech conversion\n",
    "\n",
    "- Digital assistants\n",
    "\n",
    "- Near-human-level autonomous driving\n",
    "\n",
    "- Improved ad targeting\n",
    "\n",
    "- Improved search results\n",
    "\n",
    "- Ability to answer natural-language questions\n",
    "\n",
    "- Superhuman Go playing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 Don't Believe the Short-Term Hype\n",
    "\n",
    "Most expectations tend to be higher than is actually possible in the near future. Eventually progress will slow again, like it did in the 60s and 90s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 The Promise of AI\n",
    "\n",
    "Long-term, future is looking bright, especially as people begin to recognize the possibilities that AI can bring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Before Deep Learning: A Brief History of Machine Learning\n",
    "\n",
    "Deep learning isn't always the right tool for the job - sometimes there isn't enough data, sometimes the problem is better solved with a different algorithm. \n",
    "\n",
    "Learn different approaches so you don't try to tackle every machine learning problem with deep learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Probabilistic Thinking\n",
    "\n",
    "\n",
    "Probabilistic modeling is the application of the principles of statistics to data analysis. It was one of the earliest forms of machine learning, and it’s still widely used to this day. One of the best-known algorithms in this category is the Naive Bayes algorithm.\n",
    "\n",
    "Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theorem while assuming that the features in the input data are all independent (a strong, or “naive” assumption, which is where the name comes from). Bayes’ theorem and the foundations of statistics date back to the eighteenth century, and these are all you need to start using Naive Bayes classifiers.\n",
    "\n",
    "A closely related model is the logistic regression (logreg for short), which is sometimes considered to be the “hello world” of modern machine learning. Don’t be misled by its name—logreg is a classification algorithm rather than a regression\n",
    "algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Early Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "Early iterations of neural networks have been completely supplanted by the modern variants covered in these pages, but it’s helpful to be aware of how deep learning originated. Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to get started. For a long time, the missing piece was an efficient way to train large neural networks. \n",
    "\n",
    "\n",
    "This changed in the mid-1980s, when multiple people independently rediscovered the Backpropagation algorithm — a way to train chains of parametric operations using gradient-descent optimization and started applying it to neural networks.\n",
    "\n",
    "\n",
    "The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits. The resulting network, dubbed LeNet, was used by the United States Postal Service in the 1990s to automate the reading of ZIP codes on mail envelopes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Kernel Methods\n",
    "\n",
    "\n",
    "Kernel methods are a group of classification algorithms, the best known of which is the support vector machine (SVM).\n",
    "\n",
    "\n",
    "SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories. A decision boundary can be thought of as a line or surface separating your training data into two spaces corresponding to two categories. To classify new data points, you just need to check which side of the decision boundary they fall on.\n",
    "\n",
    "\n",
    "\n",
    "![Decision Boundary](Images/01_08.jpg)\n",
    "\n",
    "\n",
    "\n",
    "SVMs find these boundaries in two steps:\n",
    "\n",
    "- The data is mapped to a new high-dimensional representation where the decision boundary can be expressed as a hyperplane (if the data was two-dimensional, a hyperplane would be a straight line).\n",
    "\n",
    "- A good decision boundary (a separation hyperplane) is computed by trying to maximize the distance between the hyperplane and the closest data points from each class, a step called maximizing the margin. This allows the boundary to generalize well to new samples outside of the training dataset.\n",
    "\n",
    "\n",
    "\n",
    "**The kernel trick** - to find a good decision hyperplanes in the new representation space, you don't have to explicitly compute the coordinates of your points in the new space; you just need to compute the distance between two points in that space, which can be done efficiently using a kernel function. \n",
    "\n",
    "A kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target representation space, completely bypassing the explicit computation of the new representation. Kernel functions are typically crafted by hand rather than learned from data - in the case of an SVM, only the separation hyperplane is learned.\n",
    "\n",
    "SVMs were popular, but proved hard to scale to large datasets and didn't provide good results for perceptual problems such as image classification. Because an SVM is a shallow method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Decision Trees, Random Forests, and Gradient Boosting Machines\n",
    "\n",
    "\n",
    "\n",
    "![1.2.4](Images/01_09.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Back to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 What Makes Deep Learning Different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 The Modern Machine-Learning Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Why Deep Learning? Why Now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 A New Wave of Investment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 The Democratization of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Will It Last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
