{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4 - Fundamentals of Machine Learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Four Branches of Machine Learning\n",
    "\n",
    "Machine learning algorithms generally fall into four broad categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Supervised Learning\n",
    "\n",
    "Supervised learning consists of learning to map input data to known targets (also called **annotations**), given a set of examples.\n",
    "\n",
    "Supervised learning mostly consists of classification and regression, but there are also the following:\n",
    "\n",
    "- **Sequence generation**: Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems (such as repeatedly predicting a word or token in a sequence).\n",
    "\n",
    "- **Syntax tree prediction**: Given a sentence, predict its decomposition into a syntax tree.\n",
    "\n",
    "- **Object detection**: Given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem (given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression.\n",
    "\n",
    "- **Image segmentation**: Given a picture, draw a pixel-level mask on a specific object.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Unsupervised Learning\n",
    "\n",
    "This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand.\n",
    "\n",
    "Unsupervised learning is often a necessary step in better understanding a dataset before attempting to solve a supervised learning problem.\n",
    "\n",
    "**Dimensionality reduction** and **clustering** are well-known categories of unsupervised learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Self-supervised Learning \n",
    "\n",
    "Self-supervised learning is supervised learning without human-annotated labels (or supervised learning without humans). \n",
    "\n",
    "There are still labels involved (the learning has to be supervised by something), but they are generated from the input data, typically using a heuristic algorithm.\n",
    "\n",
    "**Autoencoders** are a well-known instance of self-supervised learning, where the generated targets are the input, unmodified.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, an **agent** receives information about its environment and learns to choose actions that will maximize some reward. For instance, a neural network that \"looks\" at a video game screen and outputs game actions in order to maximize its score can be trained via reinforcement learning.\n",
    "\n",
    "This branch of machine learning recently started to get a lot of attention after Google DeepMind successfully applied it to learning to play Atari games (and, later, learning to play Go at the highest level).\n",
    "\n",
    "\n",
    "Currently, reinforcement learning is mostly a research area and hasn’t yet had sig- nificant practical successes beyond games. In time, however, we expect to see rein- forcement learning take over an increasingly large range of real-world applications: self-driving cars, robotics, resource management, education, and so on.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Classification and Regression Glossary \n",
    "\n",
    "\n",
    "- **Sample** or **input**: One data point that goes into your model.\n",
    "\n",
    "- **Prediction** or **output**: What comes out of your model.\n",
    "\n",
    "- **Target**: The truth. What your model should ideally have predicted, according to an external source of data.\n",
    "\n",
    "- **Prediction error** or **loss value**: A measure of the distance between your model’s prediction and the target.\n",
    "\n",
    "- **Classes**: A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, “dog” and “cat” are the two classes.\n",
    "\n",
    "- **Label**: A specific instance of a class annotation in a classification problem. For instance, if picture #1234 is annotated as containing the class “dog,” then “dog” is a label of picture #1234.\n",
    "\n",
    "- **Ground-truth** or **annotations**: All targets for a dataset, typically collected by humans.\n",
    "\n",
    "- **Binary classification**: A classification task where each input sample should be categorized into two exclusive categories.\n",
    "\n",
    "- **Multiclass classification**: A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.\n",
    "\n",
    "- **Multilabel classification**: A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the “dog” label. The number of labels per image is usually variable.\n",
    "\n",
    "- **Scalar regression**: A task where the target is a continuous scalar value. Pre- dicting house prices is a good example: the different target prices form a con- tinuous space.\n",
    "\n",
    "- **Vector regression**: A task where the target is a set of continuous values: for example, a continuous vector. If you’re doing regression against multiple val- ues (such as the coordinates of a bounding box in an image), then you’re doing vector regression.\n",
    "\n",
    "- **Mini-batch** or **batch**: A small set of samples (typically between 8 and 128) that are processed simultaneously by the model. The number of samples is often a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch is used to compute a single gradient-descent update applied to the weights of the model.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating Machine Learning Models\n",
    "\n",
    "\n",
    "In machine learning, the goal is to achieve models that **generalize** - that perform well on never-before-seen data - and overfitting is the central obstacle. \n",
    "\n",
    "Split the data into a training set, a validation set, and a test set. The reason to not evaluate the models on the same data they were trained on is because the model will begin to **overfit**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Training, Validation, and Test Sets \n",
    "\n",
    "You train on the training data, evaluate your model on the validation data, and once your model is ready, test it a final time on the test data.\n",
    "\n",
    "\n",
    "\n",
    "The reason is that developing a model always involves tuning its configuration: for example, choosing the number of layers or the size of the layers (called the **hyper-parameters** of the model, to distinguish them from the **parameters**, which are the network’s weights). You do this tuning by using as a feedback signal the performance of the model on the validation data. In essence, this tuning is a form of learning: a search for a good configuration in some parameter space. As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in overfitting to the validation set, even though your model is never directly trained on it.\n",
    "\n",
    "\n",
    "\n",
    "Central to this phenomenon is the notion of information leaks. Every time you tune a hyperparameter of your model based on the model’s performance on the validation set, some information about the validation data leaks into the model. If you do this only once, for one parameter, then very few bits of information will leak, and your validation set will remain reliable to evaluate the model. But if you repeat this many times — running one experiment, evaluating on the validation set, and modifying your model as a result — then you’ll leak an increasingly significant amount of information about the validation set into the model.\n",
    "\n",
    "\n",
    "\n",
    "At the end of the day, you’ll end up with a model that performs artificially well on the validation data, because that’s what you optimized it for. You care about performance on completely new data, not the validation data, so you need to use a completely different, never-before-seen dataset to evaluate the model: the test dataset. Your model shouldn’t have had access to any information about the test set, even indirectly.\n",
    "\n",
    "\n",
    "\n",
    "Splitting your data into training, validation, and test sets may seem straightforward, but there are a few advanced ways to do it that can come in handy when little data is available. Let’s review three classic evaluation recipes: simple hold-out validation, K- fold validation, and iterated K-fold validation with shuffling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Hold-Out Validation\n",
    "\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. In order to prevent information leaks, you shouldn’t tune your model based on the test set, and therefore you should also reserve a validation set.\n",
    "\n",
    "\n",
    "![HoldOutValidation1](Images/04_02.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Implementation:*\n",
    "\n",
    "![HoldOutValidation](Images/04_01.jpg)\n",
    "\n",
    "\n",
    "\n",
    "This is the simplest evaluation protocol, and it suffers from one flaw: if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand. This is easy to recognize: if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance, then you’re having this issue.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Validation\n",
    "\n",
    "With this approach, you split your data into K partitions of equal size. For each partition i, train a model on the remaining K – 1 partitions, and evaluate it on partition i. Your final score is then the averages of the K scores obtained. This method is helpful when the performance of your model shows significant variance based on your train-test split. Like hold-out validation, this method doesn’t exempt you from using a distinct validation set for model calibration.\n",
    "\n",
    "\n",
    "\n",
    "![KFold](Images/04_03.jpg)\n",
    "\n",
    "\n",
    "\n",
    "*Implementation:*\n",
    "\n",
    "\n",
    "\n",
    "![KFold](Images/04_04.jpg)\n",
    "\n",
    "\n",
    "\n",
    "![KFold](Images/04_05.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterated K-Fold Validation with Shuffling\n",
    "\n",
    "\n",
    "This one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in Kaggle competitions. It consists of applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training and evaluating P × K models (where P is the number of iterations you use), which can very expensive.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Things to Keep in Mind\n",
    "\n",
    "\n",
    "When choosing an evaluation protocol:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Data representativeness**: You want both your training set and your testing set to be representative of the data at hand. For instance, if you’re trying to classify images of digits, and you’re starting from an array of samples where the samples are ordered by their class, taking the first 80% of the array as your training set and the remaining 20% as your test set will result in your training set containing only classes 0–7, whereas your test set contains only classes 8–9. This seems like a ridiculous mistake, but it’s surprisingly common. For this reason, you usually should randomly shuffle your data before splitting it into training and test sets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **The arrow of time**: If you’re trying to predict the future given the past (for example, tomorrow’s weather, stock movements, and so on), you should not randomly shuffle your data before splitting it, because doing so will create a **temporal leak**: your model will effectively be trained on data from the future. In such situations, you should always make sure all data in your test set is **posterior** to the data in the training set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Redundancy in your data**: If some data points in your data appear twice (fairly common with real-world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets. In effect, you’ll be testing on part of your training data, which is the worst thing you can do! Make sure your training set and validation set are disjoint.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Preprocessing, Feature Engineering, and Feature Learning\n",
    "\n",
    "\n",
    "pg.124\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
